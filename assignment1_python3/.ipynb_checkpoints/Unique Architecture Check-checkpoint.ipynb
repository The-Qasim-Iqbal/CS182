{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side notebook used to run additional models at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deeplearning.classifiers.cnn import *\n",
    "from deeplearning.classifiers.convnet import *\n",
    "from deeplearning.data_utils import get_CIFAR10_data\n",
    "from deeplearning.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from deeplearning.layers import *\n",
    "from deeplearning.fast_layers import *\n",
    "from deeplearning.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning/datasets/cifar-10-batches-py/data_batch_1\n",
      "deeplearning/datasets/cifar-10-batches-py/data_batch_2\n",
      "deeplearning/datasets/cifar-10-batches-py/data_batch_3\n",
      "deeplearning/datasets/cifar-10-batches-py/data_batch_4\n",
      "deeplearning/datasets/cifar-10-batches-py/data_batch_5\n",
      "deeplearning/datasets/cifar-10-batches-py/test_batch\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "    print ('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished init\n",
      "(Iteration 1 / 7650) loss: 0.617190\n",
      "(Epoch 0 / 10) train acc: 0.843000; val_acc: 0.663000\n",
      "(Iteration 51 / 7650) loss: 0.476354\n",
      "(Iteration 101 / 7650) loss: 0.548005\n",
      "(Iteration 151 / 7650) loss: 0.867544\n",
      "(Iteration 201 / 7650) loss: 0.497629\n",
      "(Iteration 251 / 7650) loss: 0.730104\n",
      "(Iteration 301 / 7650) loss: 0.604761\n",
      "(Iteration 351 / 7650) loss: 0.620686\n",
      "(Iteration 401 / 7650) loss: 0.489007\n",
      "(Iteration 451 / 7650) loss: 0.740354\n",
      "(Iteration 501 / 7650) loss: 0.604951\n",
      "(Iteration 551 / 7650) loss: 0.878732\n",
      "(Iteration 601 / 7650) loss: 0.684641\n",
      "(Iteration 651 / 7650) loss: 0.683603\n",
      "(Iteration 701 / 7650) loss: 0.609198\n",
      "(Iteration 751 / 7650) loss: 0.628533\n",
      "(Epoch 1 / 10) train acc: 0.842000; val_acc: 0.680000\n",
      "(Iteration 801 / 7650) loss: 0.562894\n",
      "(Iteration 851 / 7650) loss: 0.595498\n",
      "(Iteration 901 / 7650) loss: 0.488428\n",
      "(Iteration 951 / 7650) loss: 0.459113\n",
      "(Iteration 1001 / 7650) loss: 0.542215\n",
      "(Iteration 1051 / 7650) loss: 0.608849\n",
      "(Iteration 1101 / 7650) loss: 0.567404\n",
      "(Iteration 1151 / 7650) loss: 0.829071\n",
      "(Iteration 1201 / 7650) loss: 0.840115\n",
      "(Iteration 1251 / 7650) loss: 0.838620\n",
      "(Iteration 1301 / 7650) loss: 0.587982\n",
      "(Iteration 1351 / 7650) loss: 0.507750\n",
      "(Iteration 1401 / 7650) loss: 0.776073\n",
      "(Iteration 1451 / 7650) loss: 0.582323\n",
      "(Iteration 1501 / 7650) loss: 0.713310\n",
      "(Epoch 2 / 10) train acc: 0.854000; val_acc: 0.668000\n",
      "(Iteration 1551 / 7650) loss: 0.531653\n",
      "(Iteration 1601 / 7650) loss: 0.674253\n",
      "(Iteration 1651 / 7650) loss: 0.519027\n",
      "(Iteration 1701 / 7650) loss: 0.561432\n",
      "(Iteration 1751 / 7650) loss: 0.609501\n",
      "(Iteration 1801 / 7650) loss: 0.653305\n",
      "(Iteration 1851 / 7650) loss: 0.578200\n",
      "(Iteration 1901 / 7650) loss: 0.497590\n",
      "(Iteration 1951 / 7650) loss: 0.647805\n",
      "(Iteration 2001 / 7650) loss: 0.504691\n",
      "(Iteration 2051 / 7650) loss: 0.627937\n",
      "(Iteration 2101 / 7650) loss: 0.581253\n",
      "(Iteration 2151 / 7650) loss: 0.535255\n",
      "(Iteration 2201 / 7650) loss: 0.631573\n",
      "(Iteration 2251 / 7650) loss: 0.465471\n",
      "(Epoch 3 / 10) train acc: 0.878000; val_acc: 0.663000\n",
      "(Iteration 2301 / 7650) loss: 0.490443\n",
      "(Iteration 2351 / 7650) loss: 0.528492\n",
      "(Iteration 2401 / 7650) loss: 0.508881\n",
      "(Iteration 2451 / 7650) loss: 0.495033\n",
      "(Iteration 2501 / 7650) loss: 0.473715\n",
      "(Iteration 2551 / 7650) loss: 0.596447\n",
      "(Iteration 2601 / 7650) loss: 0.681685\n",
      "(Iteration 2651 / 7650) loss: 0.525126\n",
      "(Iteration 2701 / 7650) loss: 0.548459\n",
      "(Iteration 2751 / 7650) loss: 0.640065\n",
      "(Iteration 2801 / 7650) loss: 0.513127\n",
      "(Iteration 2851 / 7650) loss: 0.551586\n",
      "(Iteration 2901 / 7650) loss: 0.584176\n",
      "(Iteration 2951 / 7650) loss: 0.573408\n",
      "(Iteration 3001 / 7650) loss: 0.396389\n",
      "(Iteration 3051 / 7650) loss: 0.594386\n",
      "(Epoch 4 / 10) train acc: 0.873000; val_acc: 0.653000\n",
      "(Iteration 3101 / 7650) loss: 0.568430\n",
      "(Iteration 3151 / 7650) loss: 0.483529\n",
      "(Iteration 3201 / 7650) loss: 0.472378\n",
      "(Iteration 3251 / 7650) loss: 0.474889\n",
      "(Iteration 3301 / 7650) loss: 0.482456\n",
      "(Iteration 3351 / 7650) loss: 0.555377\n",
      "(Iteration 3401 / 7650) loss: 0.367218\n",
      "(Iteration 3451 / 7650) loss: 0.482986\n",
      "(Iteration 3501 / 7650) loss: 0.626468\n",
      "(Iteration 3551 / 7650) loss: 0.605970\n",
      "(Iteration 3601 / 7650) loss: 0.323622\n",
      "(Iteration 3651 / 7650) loss: 0.601669\n",
      "(Iteration 3701 / 7650) loss: 0.642574\n",
      "(Iteration 3751 / 7650) loss: 0.530494\n",
      "(Iteration 3801 / 7650) loss: 0.812359\n",
      "(Epoch 5 / 10) train acc: 0.907000; val_acc: 0.663000\n",
      "(Iteration 3851 / 7650) loss: 0.375460\n",
      "(Iteration 3901 / 7650) loss: 0.546077\n",
      "(Iteration 3951 / 7650) loss: 0.375964\n",
      "(Iteration 4001 / 7650) loss: 0.482023\n",
      "(Iteration 4051 / 7650) loss: 0.587373\n",
      "(Iteration 4101 / 7650) loss: 0.624170\n",
      "(Iteration 4151 / 7650) loss: 0.537647\n",
      "(Iteration 4201 / 7650) loss: 0.752712\n",
      "(Iteration 4251 / 7650) loss: 0.440868\n",
      "(Iteration 4301 / 7650) loss: 0.501413\n",
      "(Iteration 4351 / 7650) loss: 0.439254\n",
      "(Iteration 4401 / 7650) loss: 0.486038\n",
      "(Iteration 4451 / 7650) loss: 0.479383\n",
      "(Iteration 4501 / 7650) loss: 0.508778\n",
      "(Iteration 4551 / 7650) loss: 0.514397\n",
      "(Epoch 6 / 10) train acc: 0.903000; val_acc: 0.674000\n",
      "(Iteration 4601 / 7650) loss: 0.480207\n",
      "(Iteration 4651 / 7650) loss: 0.713991\n",
      "(Iteration 4701 / 7650) loss: 0.402776\n",
      "(Iteration 4751 / 7650) loss: 0.608004\n",
      "(Iteration 4801 / 7650) loss: 0.628661\n",
      "(Iteration 4851 / 7650) loss: 0.340693\n",
      "(Iteration 4901 / 7650) loss: 0.422867\n",
      "(Iteration 4951 / 7650) loss: 0.575379\n",
      "(Iteration 5001 / 7650) loss: 0.466342\n",
      "(Iteration 5051 / 7650) loss: 0.507230\n",
      "(Iteration 5101 / 7650) loss: 0.549922\n",
      "(Iteration 5151 / 7650) loss: 0.783513\n",
      "(Iteration 5201 / 7650) loss: 0.467846\n",
      "(Iteration 5251 / 7650) loss: 0.557426\n",
      "(Iteration 5301 / 7650) loss: 0.490603\n",
      "(Iteration 5351 / 7650) loss: 0.488572\n",
      "(Epoch 7 / 10) train acc: 0.914000; val_acc: 0.679000\n",
      "(Iteration 5401 / 7650) loss: 0.504749\n",
      "(Iteration 5451 / 7650) loss: 0.478757\n",
      "(Iteration 5501 / 7650) loss: 0.544907\n",
      "(Iteration 5551 / 7650) loss: 0.490932\n",
      "(Iteration 5601 / 7650) loss: 0.588300\n",
      "(Iteration 5651 / 7650) loss: 0.551670\n",
      "(Iteration 5701 / 7650) loss: 0.566441\n",
      "(Iteration 5751 / 7650) loss: 0.558439\n",
      "(Iteration 5801 / 7650) loss: 0.431782\n",
      "(Iteration 5851 / 7650) loss: 0.410330\n",
      "(Iteration 5901 / 7650) loss: 0.474891\n",
      "(Iteration 5951 / 7650) loss: 0.485491\n",
      "(Iteration 6001 / 7650) loss: 0.534304\n",
      "(Iteration 6051 / 7650) loss: 0.533709\n",
      "(Iteration 6101 / 7650) loss: 0.652150\n",
      "(Epoch 8 / 10) train acc: 0.905000; val_acc: 0.675000\n",
      "(Iteration 6151 / 7650) loss: 0.577248\n",
      "(Iteration 6201 / 7650) loss: 0.571979\n",
      "(Iteration 6251 / 7650) loss: 0.510741\n",
      "(Iteration 6301 / 7650) loss: 0.485386\n",
      "(Iteration 6351 / 7650) loss: 0.446898\n",
      "(Iteration 6401 / 7650) loss: 0.497032\n",
      "(Iteration 6451 / 7650) loss: 0.620540\n",
      "(Iteration 6501 / 7650) loss: 0.446272\n",
      "(Iteration 6551 / 7650) loss: 0.445794\n",
      "(Iteration 6601 / 7650) loss: 0.494471\n",
      "(Iteration 6651 / 7650) loss: 0.676784\n",
      "(Iteration 6701 / 7650) loss: 0.463305\n",
      "(Iteration 6751 / 7650) loss: 0.413158\n",
      "(Iteration 6801 / 7650) loss: 0.389138\n",
      "(Iteration 6851 / 7650) loss: 0.346715\n",
      "(Epoch 9 / 10) train acc: 0.899000; val_acc: 0.668000\n",
      "(Iteration 6901 / 7650) loss: 0.481467\n",
      "(Iteration 6951 / 7650) loss: 0.472576\n",
      "(Iteration 7001 / 7650) loss: 0.561494\n",
      "(Iteration 7051 / 7650) loss: 0.540760\n",
      "(Iteration 7101 / 7650) loss: 0.488116\n",
      "(Iteration 7151 / 7650) loss: 0.432424\n",
      "(Iteration 7201 / 7650) loss: 0.347665\n",
      "(Iteration 7251 / 7650) loss: 0.499825\n",
      "(Iteration 7301 / 7650) loss: 0.452437\n",
      "(Iteration 7351 / 7650) loss: 0.489386\n",
      "(Iteration 7401 / 7650) loss: 0.485003\n",
      "(Iteration 7451 / 7650) loss: 0.488805\n",
      "(Iteration 7501 / 7650) loss: 0.450448\n",
      "(Iteration 7551 / 7650) loss: 0.454532\n",
      "(Iteration 7601 / 7650) loss: 0.400182\n",
      "(Epoch 10 / 10) train acc: 0.921000; val_acc: 0.659000\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(35)\n",
    "# unique_model2 = BestConvNet2(weight_scale=.001, hidden_dim=100, reg=.1, dropout=0.5, filter_size = 5, num_filters=32)\n",
    "# unique_solver2 = Solver(unique_model, data,\n",
    "#     num_epochs=10, batch_size=64,\n",
    "#     update_rule='adam',\n",
    "#     optim_config={\n",
    "#       'learning_rate': .0001,\n",
    "#     },\n",
    "#     verbose=True, print_every=50)\n",
    "# unique_solver2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy:  0.67\n",
      "Test set accuracy:  0.674\n"
     ]
    }
   ],
   "source": [
    "# best_model = unique_model\n",
    "# y_test_pred = np.argmax(best_model.loss(X_test), axis=1)\n",
    "# y_val_pred = np.argmax(best_model.loss(X_val), axis=1)\n",
    "# print ('Validation set accuracy: ', (y_val_pred == y_val).mean())\n",
    "# print ('Test set accuracy: ', (y_test_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished init\n",
      "(Iteration 1 / 6120) loss: 0.270635\n",
      "(Epoch 0 / 8) train acc: 0.823000; val_acc: 0.677000\n",
      "(Iteration 51 / 6120) loss: 1.019592\n",
      "(Iteration 101 / 6120) loss: 0.663527\n",
      "(Iteration 151 / 6120) loss: 0.612975\n",
      "(Iteration 201 / 6120) loss: 0.594992\n",
      "(Iteration 251 / 6120) loss: 0.566160\n",
      "(Iteration 301 / 6120) loss: 0.536942\n",
      "(Iteration 351 / 6120) loss: 0.747610\n",
      "(Iteration 401 / 6120) loss: 0.750372\n",
      "(Iteration 451 / 6120) loss: 0.538336\n",
      "(Iteration 501 / 6120) loss: 0.477159\n",
      "(Iteration 551 / 6120) loss: 0.656102\n",
      "(Iteration 601 / 6120) loss: 0.523764\n",
      "(Iteration 651 / 6120) loss: 0.678561\n",
      "(Iteration 701 / 6120) loss: 0.604308\n",
      "(Iteration 751 / 6120) loss: 0.685651\n",
      "(Epoch 1 / 8) train acc: 0.875000; val_acc: 0.663000\n",
      "(Iteration 801 / 6120) loss: 0.462217\n",
      "(Iteration 851 / 6120) loss: 0.735112\n",
      "(Iteration 901 / 6120) loss: 0.442382\n",
      "(Iteration 951 / 6120) loss: 0.574936\n",
      "(Iteration 1001 / 6120) loss: 0.639210\n",
      "(Iteration 1051 / 6120) loss: 0.662615\n",
      "(Iteration 1101 / 6120) loss: 0.530449\n",
      "(Iteration 1151 / 6120) loss: 0.601964\n",
      "(Iteration 1201 / 6120) loss: 0.808707\n",
      "(Iteration 1251 / 6120) loss: 0.545687\n",
      "(Iteration 1301 / 6120) loss: 0.572471\n",
      "(Iteration 1351 / 6120) loss: 0.503112\n",
      "(Iteration 1401 / 6120) loss: 0.662106\n",
      "(Iteration 1451 / 6120) loss: 0.627056\n",
      "(Iteration 1501 / 6120) loss: 0.573331\n",
      "(Epoch 2 / 8) train acc: 0.876000; val_acc: 0.658000\n",
      "(Iteration 1551 / 6120) loss: 0.621843\n",
      "(Iteration 1601 / 6120) loss: 0.593601\n",
      "(Iteration 1651 / 6120) loss: 0.508920\n",
      "(Iteration 1701 / 6120) loss: 0.674457\n",
      "(Iteration 1751 / 6120) loss: 0.633130\n",
      "(Iteration 1801 / 6120) loss: 0.551375\n",
      "(Iteration 1851 / 6120) loss: 0.617026\n",
      "(Iteration 1901 / 6120) loss: 0.612646\n",
      "(Iteration 1951 / 6120) loss: 0.524782\n",
      "(Iteration 2001 / 6120) loss: 0.646277\n",
      "(Iteration 2051 / 6120) loss: 0.657148\n",
      "(Iteration 2101 / 6120) loss: 0.613655\n",
      "(Iteration 2151 / 6120) loss: 0.527004\n",
      "(Iteration 2201 / 6120) loss: 0.606576\n",
      "(Iteration 2251 / 6120) loss: 0.615950\n",
      "(Epoch 3 / 8) train acc: 0.884000; val_acc: 0.674000\n",
      "(Iteration 2301 / 6120) loss: 0.603938\n",
      "(Iteration 2351 / 6120) loss: 0.535281\n",
      "(Iteration 2401 / 6120) loss: 0.612015\n",
      "(Iteration 2451 / 6120) loss: 0.511671\n",
      "(Iteration 2501 / 6120) loss: 0.447328\n",
      "(Iteration 2551 / 6120) loss: 0.467489\n",
      "(Iteration 2601 / 6120) loss: 0.580249\n",
      "(Iteration 2651 / 6120) loss: 0.650490\n",
      "(Iteration 2701 / 6120) loss: 0.572198\n",
      "(Iteration 2751 / 6120) loss: 0.548472\n",
      "(Iteration 2801 / 6120) loss: 0.683084\n",
      "(Iteration 2851 / 6120) loss: 0.599684\n",
      "(Iteration 2901 / 6120) loss: 0.578967\n",
      "(Iteration 2951 / 6120) loss: 0.490815\n",
      "(Iteration 3001 / 6120) loss: 0.524132\n",
      "(Iteration 3051 / 6120) loss: 0.611599\n",
      "(Epoch 4 / 8) train acc: 0.897000; val_acc: 0.690000\n",
      "(Iteration 3101 / 6120) loss: 0.504562\n",
      "(Iteration 3151 / 6120) loss: 0.609630\n",
      "(Iteration 3201 / 6120) loss: 0.598929\n",
      "(Iteration 3251 / 6120) loss: 0.675511\n",
      "(Iteration 3301 / 6120) loss: 0.556325\n",
      "(Iteration 3351 / 6120) loss: 0.471288\n",
      "(Iteration 3401 / 6120) loss: 0.500905\n",
      "(Iteration 3451 / 6120) loss: 0.613834\n",
      "(Iteration 3501 / 6120) loss: 0.435276\n",
      "(Iteration 3551 / 6120) loss: 0.569453\n",
      "(Iteration 3601 / 6120) loss: 0.666140\n",
      "(Iteration 3651 / 6120) loss: 0.648277\n",
      "(Iteration 3701 / 6120) loss: 0.467083\n",
      "(Iteration 3751 / 6120) loss: 0.563688\n",
      "(Iteration 3801 / 6120) loss: 0.449274\n",
      "(Epoch 5 / 8) train acc: 0.895000; val_acc: 0.668000\n",
      "(Iteration 3851 / 6120) loss: 0.397778\n",
      "(Iteration 3901 / 6120) loss: 0.544477\n",
      "(Iteration 3951 / 6120) loss: 0.657742\n",
      "(Iteration 4001 / 6120) loss: 0.482561\n",
      "(Iteration 4051 / 6120) loss: 0.461566\n",
      "(Iteration 4101 / 6120) loss: 0.602677\n",
      "(Iteration 4151 / 6120) loss: 0.527150\n",
      "(Iteration 4201 / 6120) loss: 0.490869\n",
      "(Iteration 4251 / 6120) loss: 0.529742\n",
      "(Iteration 4301 / 6120) loss: 0.447707\n",
      "(Iteration 4351 / 6120) loss: 0.482472\n",
      "(Iteration 4401 / 6120) loss: 0.664311\n",
      "(Iteration 4451 / 6120) loss: 0.512755\n",
      "(Iteration 4501 / 6120) loss: 0.572381\n",
      "(Iteration 4551 / 6120) loss: 0.437542\n",
      "(Epoch 6 / 8) train acc: 0.899000; val_acc: 0.666000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1bb87660a07c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     },\n\u001b[1;32m      9\u001b[0m     verbose=True, print_every=50)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0munique_solver2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/assignment1_python3/deeplearning/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/assignment1_python3/deeplearning/solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/assignment1_python3/deeplearning/classifiers/convnet.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# variable.                                                                #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrp_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu_pool_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabn_cache1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_bn_relu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mout3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabn_cache2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_bn_relu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/assignment1_python3/deeplearning/layer_utils.py\u001b[0m in \u001b[0;36mconv_relu_pool_forward\u001b[0;34m(x, w, b, conv_param, pool_param)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool_forward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/assignment1_python3/deeplearning/fast_layers.py\u001b[0m in \u001b[0;36mmax_pool_forward_fast\u001b[0;34m(x, pool_param)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mtiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpool_height\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpool_width\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msame_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool_forward_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/assignment1_python3/deeplearning/fast_layers.py\u001b[0m in \u001b[0;36mmax_pool_forward_reshape\u001b[0;34m(x, pool_param)\u001b[0m\n\u001b[1;32m    186\u001b[0m     x_reshaped = x.reshape(N, C, H // pool_height, pool_height,\n\u001b[1;32m    187\u001b[0m                            W // pool_width, pool_width)\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     28\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     29\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# np.random.seed(35)\n",
    "# unique_model2 = BestConvNet2(weight_scale=.001, hidden_dim=125, reg=.1, dropout=0.5, filter_size = 5, num_filters=32)\n",
    "# unique_solver2 = Solver(unique_model2, data,\n",
    "#     num_epochs=8, batch_size=64,\n",
    "#     update_rule='adam',\n",
    "#     optim_config={\n",
    "#       'learning_rate': .0001,\n",
    "#     },\n",
    "#     verbose=True, print_every=50)\n",
    "# unique_solver2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
